--- 
title: "A handbook for Computational Genetics"
author: "Alfred Pozarickij"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-dem
description: "This is the book describing theoretical and practical approaches in analysis of genome data."
---

# Preface {-}

The scope of this book is to provide an outline of computational methods available for the analysis of genetic data.

First chapter introduces methods to infer population parameters. 

Next X chapters focus on population genetics.

In this book, the amount of mathematics and statistics is kept to a minimum. Only methods designed specifically to address issues in genetics are shown. I have produced another book [insert link here], which is intended to familiarise the reader with commonly used approaches and develop some intuition behind them. By no means the list is comprehensive and only serves as a quick guide. The internet provides much more information regarding this topic.

Rather than providing references at the end of each chapter, I decided to combine them into supplementary text [insert link here]. References are arranged according to different topics discussed in this book. I tried to do my best to cause as little confusion as possible.

Finally, don't hesitate to contact me if I have not included your favorite method (apozarickij@gmail.com). I would be more than happy to hear about it.








<!--chapter:end:index.Rmd-->

# (PART) Quantitative Genetics {-}

# Population parameters

## Mean

## Variance

## Covariance

## Genetic correlation

## Additivity

## Dominance/Recesivness

## Codominance

## Infinitesimal model

### Omnigenic model

## Henetic relationships by Malecot

## Genotype simulations

## Phenotype simulations

<!--chapter:end:01-Descriptives.Rmd-->

# Sequencing technologies

The first step in any genetic analysis is to map sequence reads, callibrate base qualities, and call variants. 

Prior to mapping, evaluate base composition along reads. Calculate the proportion of A, C, G, T bases along each read. Flag runs with evidence of unusual patterns of base composition compared to the target genome.
Evaluate machine quality scores along reads. Calculate average quality scores per position. Flag runs with evidence of unusual quality score distributions.
Calculate the input number of reads and number of bases for each sequenced sample.

## While whole genome sequencing and re-sequencing represent ~90% of all DNA based sequencing applications, it’s important to not lose sight of the myriad of new protocols available to count or detect epi-genomic features. These include genotyping, measuring DNA-protein interactions and epigenetic markers. Several examples of these protocols are listed below:

### DNA-protein interactions
DNAse-Seq
MNAse-Seq
X-ChIP
ChIP-Seq
FAIRE-Seq
ATAC-Seq
Chia-PET
Hi-C
3-C, 4-C, 5-C
Capture-C
HiTS-FLIP

### Epigenetics
Bisulfite-Seq
Methyl-Seq
RRBS
PBAT
Me-DIP
oxBS-Seq
TAB-Seq
MBDCap-Seq
BisChIP-Seq

### Genotyping
RAD-Seq
ddRAD-Seq
nextRAD
Capture-Seq
ezRAD
Low input DNA-Seq
MDA
DOP-PCR
Os-Seq
MALBAC
Nuc-Seq

## Genotype calling algorithms

## Sequence alignment

Sequence alignment is a method of arranging sequences of DNA, RNA, or protein to identify regions of similarity. The similarity being identified, may be a result of functional, structural, or evolutionary relationships between the sequences.

If we compare two sequences, it is known as pairwise sequence alignment. If we compare more than two sequences, it is known as multiple sequence alignment.

## Sequence assembly

## SNP annotation

### CNV annotation

## Gene prediction



<!--chapter:end:02-Sequencing-technologies.Rmd-->

---
output:
  word_document: default
  html_document: default
  pdf_document: default
---
# Genome-wide association analysis

## DNA processing quality control

## Batch effects

https://www.bioconductor.org/packages/devel/bioc/vignettes/GWASTools/inst/doc/DataCleaning.pdf

The overall goal of this step is to check the quality of the sample batches. Substantial quality control is done by the genotyping centers prior to releasing the genotype data. However, it is possible that quality control for batches is still lower than desired. If a lower quality batch is detected then it may be necessary to re-run the genotyping for that batch. We can check the batch quality by comparing the missing call rates between batches and looking for significant allele frequency differences between batches.

### Missing call rate for samples and SNPs

The ﬁrst step is to calculate the missing call rates for each SNP and for each sample. A high missing call rate for a sample is often indicative of a poorly performing sample. It has been seen that samples from DNA that has undergone whole-genome ampliﬁcation (WGA) have a relatively higher missing call rate. Similarly a high missing call rate for a SNP is indicative of a problem SNP. Experience from the GENEVA studies has shown that there seem to be a subset of SNPs from which genotype calls are more diﬃcult to make than others. We calculate the missing call rates in a two step process: ﬁrst the missing call rates over all samples and SNPs are calculated, then the missing call rates are calculated again, ﬁltering out SNPs and samples that have an initial missing call rate greater than 0.05. The initial SNP missing call rate over all samples is saved in the SNP annotation data ﬁle as missing.n1. The analogous idea is applied to the samples: missing.e1 is saved in the sample annotation ﬁle and corresponds to the missing call rate per sample over all SNPs, excluding those SNPs with all calls missing. The missing.n2 is calculated as the call rate per SNP over all samples whose missing.e1 is less than 0.05. Again, similarly for the samples, missing.e2 is calculated for each sample over all SNPs with missing.n2 values less than 0.05. It is important to remember that the Y chromosome values should be calculated for males only, since we expect females to have no genotype values for the Y chromosome, although an occasional probe on the Y chromosome is called in a female.
If any samples have a high missing rate, we recommend further investigation of what may be causing the missing calls; the samples with a missing call rate greater than 0.05 should be ﬁltered out due to low sample quality.

### Missing call rates by batch

The missing call rate by batch is calculated to check that there are no batches with comparatively lower call rates. Usually a“batch”is a plate containing samples that were processed together through the genotyping chemistry. In this case all samples were run on diﬀerent plates (as controls for another dataset).

### Allele frequency differences across batches

In this step, the chi-square test for diﬀerences in allelic frequency is performed between each batch individually and a pool of all the other batches in the study. We then look at the mean χ<sup>2</sup> statistic over all SNPs for each batch as a function of the ethnic composition of samples in a batch.
Next we test for association between batches and population groups, using a χ<sup>2</sup> contingency test. Then we look at the relationship between the ethnic composition of each batch and the previously calculated χ<sup>2</sup> test of allelic frequency between each batch and a pool of the other batches. The point is to look for batches that diﬀer from others of similar ethnic composition, which might indicate a batch eﬀect due to genotyping artifact. In this experiment, there are only a few batches and wide variations in race among batches, so it is diﬃcult to interpret the results. In larger GWAS experiments, we generally observe a U-shaped curve of allelic frequency test statistic as a function of ethnic composition.
The χ<sup>2</sup> test is not suitable when the 2×2 tables for each SNP have very small values. For arrays in which many SNPs have very low minor allele frequency, Fisher’s exact test is more appropriate. 

## Sample quality control

### Cryptic relatedness

### Population stratification

Sometimes finding an association can be confounded by population stratification. This is because a condition may be more prevalent in one group of people than in a different group, resulting in a spurious association between the condition or trait being tested for and any genetic characteristics which vary between the two different groups of people.

While it is good practice for studies to be based on as homogeneous a group of test subjects as possible, it has been noted in [Price, 2006] that even the mild variation in genetic characteristics among those who classify themselves as belonging to one ethnic group or another can be problematic enough to confound a study done over thousands of genetic markers.

Hidden population stratification may be thought of as a non-zero F<sub>st</sub> between unknown groupings of samples.

### Heterozygosity and missingness outliers

### Differential missingness

### Sex chromosome anomalies

## Marker quality control

### Genotyping concordance

In genotyping studies where DNA is directly assayed for positions of variance, concordance is a measure of the percentage of SNPs that are measured as identical. Samples from the same individual or identical twins theoretically have a concordance of 100%, but due to assaying errors and somatic mutations, they are usually found in the range of 99% to 99.95%. Concordance can therefore be used as a method of assessing the accuracy of a genotyping assay platform.

### Mendelian errors

### Genotype call rate

### Minor allele frequency

### Hardy-Weinberg equilibrium outliers

### Additional QC for regions like MHC

### Ambigious nucleotides

If the base and target data were generated using different genotyping chips and the chromosome strand (+/-) for either is unknown, then it is not possible to match ambiguous SNPs (i.e. those with complementary alleles, either C/G or A/T) across the data sets, because it will be unknown whether the base and target data are referring to the same allele or not. While allele frequencies can be used to infer which alleles match, it is recommended to remove all ambiguous SNPs since the allele frequencies provided in base GWAS are often those from resources such as the 1000G project, and so aligning alleles according to their frequency could lead to systematic biases. 

### Non-matching nucleotides

When there is a non-ambiguous mismatch in allele coding between the data sets, such as A/C in the base and G/T in the target data, then this can be resolved by ‘flipping’ the alleles in the target data to their complementary alleles. 

### Quality control prior to meta-analysis

Allele Frequency Plots (AF Plots): looking for errors in allele frequencies and strand orientations by visually inspecting a plot of the sample allele frequency of filtered SNPs against the frequency in the 1000 Genomes phase 1 version 3 European panel3 for example.
P value vs Z-statistic Plots (PZ Plots): looking for the consistency between the reported P values and the P values implied by the coefficient estimates and standard errors in
individual cohort.
Quantile-Quantile Plots (QQ Plots): looking for the cohort-level QQ plots to look for evidence of unaccounted-for stratification.
Predicted vs Reported Standard-Error Plots (PRS Plots): making sure that the standard errors reported in individuals cohorts are approximately consistent with the reported sample size, allele frequency, and phenotype distribution. 
Use of bivariate LD score regression to verify that the estimated genetic correlations between all large cohorts (defined as N > 10,000) are large and positive.

## X-chromosome quality control

The X chromosome plays an important role in complex human traits and diseases, especially those with sexually dimorphic characteristics. Special attention needs to be given to the analysis of X due to its unique inheritance pattern and X-inactivation. These analytical complications have resulted in exclusion or mishandling of the X chromosome in the majority of genome-wide association studies (GWAS) to date.

## Single marker regression

Summary statistics can be obtained using one of the following tests: Correlation/Trend Test, Armitage Trend Test, Exact Form of Armitage Test, (Pearson) Chi-Squared Test, (Pearson) Chi-Squared Test with Yates’ Correction, Fisher’s Exact Test, Odds Ratio with Confidence Limits, Analysis of Deviance (e.g. different variance heterogeneity tests), F-Test, Logistic Regression, Linear Regression.

The estimated marginal SNP effect is

$$\LARGE \hat{\beta_j} = \left(X^T_jX_j\right)^{-1}X^T_jy\approx\frac{X^Ty}{N}$$

and its variance

$$\LARGE \sigma^2_\hat{\beta_j} = \sigma^2_r\left(X^T_jX_j\right)^{-1}\approx\frac{1}{N}$$

where $\sigma^2_r$ is the residual variance.

### Allelic test

### Genotypic test

### Additive model

Here, the genotype is coded in terms of the number of specific allele at a given locus.

### Dominant model

Here, the genotype with at least 1 copy of a specific allele at a given locus is coded as 1 and other genotypes as 0.

### Recessive model

Here, the genotype with at least 2 copies of a specific allele at a given locus is coded as 1 and other genotypes as 0.

### Categorical phenotype

### Multi-allelic GWAS

## Two-stage approach

## Haplotype GWAs design

## Joint analysis (all independent markers)

### Genomic control

In an ordinary GWAS, genomic control (GC) is used to shrink any existing inflation of the test scores (-log10 *p*-values). When testing for the single genetic effect in the GWAS, the null distribution of the test statistic for the nominal p-values is χ<sup>2</sup> with 1 degree of freedom. Since most of the SNPs are not expected to be associated with the trait, the sample distribution of the chi-squares across the genome should resemble the null distribution. If there is inflation, the chi-squares are adjusted using λ, i.e. the inflation factor estimated by comparing the distribution of the sample χ<sup>2</sup>’s and χ<sup>2</sup> distribution with 1 degree of freedom.

#### λ<sub>1000<sub>

Since λ scales with sample size, some have found it informative to report λ<sub>1000</sub>. This is equivalent to a study of 1000 cases and 1000 controls and can be calculated by rescaling λ with 1 + (λ - 1) x (1/case + 1/control) x 500, where case and control refers to the number of cases and controls respectively.

##### Stratified λ<sub>GC<sub>

Because the strength of λ<sub>GC</sub> deviation depends on allele frequency and very large sample sizes become available, it could be useful to report λ<sub>GC</sub> values based on certain MAF bins.

## Multimarker single gene-based approaches

https://cran.r-project.org/web/packages/aSPU/aSPU.pdf

## VEGAS

## Multimarker gene-set approaches (a.k.a. pathway analysis)

## fastBAT

## MAGMA

## VEGA

## Extensions to binary and categorical phenotypes

### Threshold model

## Analysis of rare variants

Check Lee et al (2014) for a review

Due to the low frequencies of rare variants, classical single marker tests commonly used in genome-wide association studies (GWAS) for studying common variants effects are not applicable.
In view of the lack of power of single marker analysis of rare variants, methods investigating rare variation are typically region-based tests where one tests for the cumulative effects of the rare variants in a region. These region-based methods can be broadly classified into three classes: burden tests, non-burden tests and hybrid of the two. The key difference between burden and non-burden tests is how the cumulative effects of the rare variants are combined for association testing. For the commonly used simple burden tests, one summarizes the rare variants within a region as a single summary genetic burden variable, e.g. the total number of rare variants in a region, and tests its association with a trait. Burden tests implicitly assume all the rare variants in the region under consideration are causal and are associated with the phenotype in the same direction and magnitude. Hence, they all share the limitation of substantial power loss when there are many non-causal genetic variants in a region and/or when there are both protective and harmful variants.
Several region-based non-burden tests have been proposed by aggregating marginal test statistics (Neale et al., 2011; Basu and Pan, 2011; Lin and Tang, 2011). One such test is the sequence kernel association test (SKAT) (Wu et al., 2011), where one summarizes the rare variants in the region using a kernel function, and then test for association with the trait of interest using a variance component score test. SKAT is robust to the signs and magnitudes of the associations of rare variants with a trait. It is more powerful than the burden tests when the effects are in different directions or the majority of variants in a region are null, but is less powerful than burden tests when most variants in a region are causal and the effects are in the same direction. Several hybrids of the two methods have been proposed to improve test power and robustness (Lee et al., 2012; Derkach et al., 2013; Sun et al., 2013).

### Collapsing methods based on pooling multiple rare variants (burden or adaptive burden tests)

#### Sum test

The most powerful multi-marker test when there are no causal variants with effects in opposite directions and when there are few or no non-causal RVs. Otherwise, it suffers from substantial loss of power.

#### Cohort Allelic Sums test (CAST)

#### Combined Multivariate Collapsing (CMC)

#### Weighted Sum test (WSS)

#### Kernel Based Adaptive Cluster (KBAC)

#### Replication Based Test (RBT)

#### ARIEL test

#### The EREC test

### Methods treating rare variant effects as random (Variance-component tests)

#### The SSU approach

Has good power in the presence of opposite association directions and small fraction of causal RVs.

#### C-alpha test

#### SKAT

Has good power in the presence of opposite association directions and non-causal RVs.
It was recently suggested that using SKAT in the presence of RVs and common variants (CVs) may be less optimal because RVs are weighted to have much more importance than CVs (Ionita-Laza et al., 2013). 

### Methods based on model selection

The model-selection approaches perform in the middle of random eﬀect and collapsing methods. One issue common to model-selection methods is that  model selection approaches use dimension-reduction strategies to substantially reduce the number of parameters one would require to ﬁt these large number of RVs. Hence, any model we can construct will never be the true model that generated the data we observe. In other words, the set of models is clearly misspeciﬁed, and model selection is best seen as a way of approximating, rather than identifying, full reality (Burnham and Anderson (2002), pp. 20-23).

#### Seq-aSum

#### Seq-aSum-VS

The Seq-aSum-VS approach classiﬁes RVs based on the direction of association (‘+1’ for positive association, ‘-1’ for negative association and ‘0’ for no association) and implements a sequential variable selection scheme to select the best model for association between the SNP-set and the disease. The only diﬀerence between the Seq-aSum approach and the Seq-aSum-VS approach is that the variable selection (‘0’ allocation for a variant) is not implemented in the former. The Seq-aSum-VS approach starts with putting all the RVs in the ‘+1’ group and proceeds by moving each RV sequentially to the other two groups and ﬁnally chooses the allocation (‘+1’,‘-1’, or ‘0’ ) with highest likelihood to the RV. The process of choosing the best model in Basu and Pan (2011)’s method can be compared to a stepwise regression, where one may not always ﬁnd the best model due to this selection scheme. This is especially true if a particular allocation results in a slightly higher likelihood than the other two allocations. In this case, choosing the allocation with highest likelihood for a SNP might not be optimal, rather it might be more eﬃcient to allow multiple allocations for a RV and construct a test that takes into account multiple plausible models for the disease-RV association. Moreover, the performance of the sequential search often depends on the ordering of the variants in this search mechanism. A model-averaging approach could potentially reduce the dependency on the ordering of the variants in this sequential search.

#### Variable Threshold Test (VT)

#### RARECOVER

#### Selective grouping method

#### Step-Up

### Combination of collapsing and random effects methods

According to Basu and Pan (2011), the model selection methods, especially Seq-aSum-VS approach, performed very well when there were both protective and deleterious causal RVs and very few non-causal RVs, but the performance of the Seq-aSum-VS approach was not very impressive in the presence of a moderate or large number of non-causal RVs. These and other ﬁndings (Basu and Pan, 2011) have led to combining the strengths of collapsing and random eﬀect methods.

#### SKAT-O

#### SKAT-C

#### Fisher method

#### MiST

### EC test

Exponentially combines score statistics. Powerful when a very small proportion of variants are causal.

### Family-based tests

https://www.omicsonline.org/open-access/literature-reviews-on-methods-for-rare-variant-association-studies-2161-0436-1000133.pdf

## Analysis of X, Y and mitochondrial chromosomes

### Dosage compensation

## Analysis of copy number variants

### Common variation

### Analysis of rare variants

## Analysis of multi-ethnic samples

The overwhelming majority of participants in current genetic studies are of European ancestry. Due to differential genetic architectures, transferability of genetic findings between populations is generally limited. Diversifying the ancestry of participants is important for the discovery of novel disease etiology. Even in large-scale European studies, causal variants might be missed if they have low frequencies or are monomorphic in European populations.

## Analysis of indirect genetic effects

## Exome analysis

## Whole-genome analysis

### Deep whole genome sequencing

Currently is applied to limited numbers of samples.
Provides most complete ascertainment of variation.

### Low coverage whole genome sequencing

Can be applied to moderate numbers of samples
Very complete ascertainment of shared variation
Less complete ascertainment of rare variants

## Analysis of multiple traits

## Mixed-model association analysis

GWAS using mixed models is appealing for several reasons:
1) More powerful in very large GWAS
2) Reduces the need for sample exclusion
3) Amplifies effective sample sizes via conditioning on polygenic predictions from genome-wide SNPs.

When using mixed models for association analysis, care must be taken to consider non-additive effects when retaining related individuals.

### EMMAX

### Fast-LMM

### GEMMA

### BOLT-LMM

When BOLT-LMM was used on very large sample sizes, analyses revealed subtleties in the interpretation of LD score regression intercepts as a means of differentiating polygenicity from confounding; the attenuation ratio was proposed to be possibly a more suitable metric as sample sizes increase.

## SAIGE

### Caveats

First, chi-squared-based tests (such as BOLT-LMM) can incur inflated type I error rates when used to analyze highly unbalanced case–control traits (case fractions <10%). There are two solutions for this.
1) Increase MAF.
2) Use saddlepoint approximation (SAIGE software).

Second, conditioning on genome-wide signal can produce loss of power under case–control ascertainment.

## Extension to unusual phenotypes

### Multinomial GWAS

### Ordinal GWAS

### Survival GWAS

## Penalized regression GWAS

## Bayesian GWAS

## Machine learning for GWAS

## Expected increase in GWAS loci with sample size

## The joint effect of genotypes over all traits

## Adjustment for winner's curse

One type of adjustment for winer's curse (using empirical Bayes), depends on the assumption that SNP effects are drawn randomly from the following mixture distribution 

## Adjustment for assortative mating

## Adjustment for attenuation bias

LD score regression

## Enrichment in candidate genes

Atwell et al. (2010) introduced a method for evaluating the enrichment of strong, but not necessarily genome-wide significant, signals for SNPs in candidate genes. An enrichment of such signals indicates that the analysis identifies true signals rather than random noise.

## GWAS resources

GWASATLAS - https://atlas.ctglab.nl/, a database of publicly available GWAS summary statistics. Each GWAS can be browsed with the manhattan plot, risk loci, MAGMA (i.e. gene-based) results, SNP heritability and genetic correlations with other GWAS in the database. 600 GWAS were performed in this project based on UK Biobank release 2 data under application ID 16406. Full summary statistics can be downloaded from the original source following the provided links.

GWAS Catalog - https://www.ebi.ac.uk/gwas/docs, the NHGRI-EBI Catalog of human genome-wide association studies.

## Reproducibility of GWAS

An open question is whether SNPs identified with genome-wide significance in earlier genome-wide association studies (GWAS) are replicated also in later GWAS conducted in biobanks. To address this question, O’Sullivan and Ioannidis examined a publicly available GWAS database and identified two, independent GWAS on the same phenotype (an earlier, “discovery” GWAS and a later, replication GWAS done in the UK biobank). The analysis evaluated 136,318,924 SNPs (of which 6,289 had reached p<5e-8 in the discovery GWAS) from 4,397,962 participants across nine phenotypes. 

Thus a trait was eligible if there were two independent GWAS available for it; one not using UKBB data (hereafter referred to as: discovery GWAS) and one using UKBB data (hereafter: replication GWAS). All discovery GWAS occurred before the replication GWAS. Further inclusion criteria was GWAS conducted in European subjects (or results available for exclusively Europeans) and GWAS with more than 50 genome-wide significant SNPs, so as to allow having a meaningful number of discoveries to be assessed for replication.

To determine the reproducibility of SNPs in the discovery and replication GWAS they performed three broad steps: 1. Determined overlap of SNPs between discovery and replication GWAS (via rsID) and only included SNPs shared between two GWAS cohorts. They then identified the SNPs that reached genome-wide significance (defined consistently as P<5e-8, regardless of the threshold that the original authors might have used) in the discovery GWAS. 2. Aligned the effect allele between the discovery and replication GWAS, and consequently inverted the effect size if effect alleles did not originally match and 3. Classified SNPs as replicated if they reached genome-wide significance (p<5e-8) in both discovery and replication GWAS and had congruent effect directions in both GWAS (e.g. odds ratio (OR) above 1 in both GWAS). All SNP effect sizes were converted to OR before reproducibility was determined via the Chinn formula (Stat. Med. 2000;19(22):3127–3131). Thus, SNP effect sizes that were originally produced from linear models for quantitative (continuous) traits were converted to OR.

They calculated the replication rate for each included trait individually, for all traits collectively, and for binary (e.g. coronary artery disease) and quantitative (e.g. diastolic blood pressure) traits separately. To calculate replication rate for each individual trait they calculated a simple proportion (e.g. [number of SNPs replicated] / [number of SNPs shared between discovery and replication GWAS]). To calculate the replication rate for all traits collectively they constructed a inverse-variance meta-analysis using fixed-effects. Further, they constructed similar inverse-variance meta-analysis to determine the replication rate for binary and quantitative traits; including only traits recorded in a binary fashion (yes/no) or on a continuous scale, respectively. To explore the replication rate across p-values and odd ratios, they also performed meta-analysis assessing the replication of SNPs with certain P-value and OR characteristics (from the discovery GWAS). They calculated the reproducibility of SNPs across the following discovery GWAS P-value categories: 5e-8 to 5e-9, 5e-9 to 5e-10, 5e-10 to 5e-11, and <5e-11. They calculated the reproducibility of SNPs across the following discovery GWAS OR categories: 1-1.05, 1.05-1.1, 1.1-1.15, 1.15-1.2, 1.2-1.3, 1.3-1.4, >1.4. 

To determine if a change in SNP effect size occurred between the earlier, discovery GWAS and the later, replication GWAS in the UKBB they constructed a single variate linear model, with the discovery OR as the predictor variable and replication OR as the outcome variable. Then, to help interpret the output from this model, they converted all OR values to above 1 (using the formula 1/OR if the original SNP OR was <1) Finally they combined SNPs across all traits for the model. From the regression model, they determined the regression coefficient for the discovery OR and interpreted this coefficient as the change in OR between GWAS (e.g. a regression coefficient of 0.80 would imply that 20% decrease in OR between discovery and replication GWAS). They only quantified the change in effect size of SNPs that were replicated, and also for all SNPs that had reached genome-wide significance in the discovery GWAS, regardless of whether they were replicated or not in the replication GWAS.

They constructed a multivariate logistic regression model to examine the association of predictors (odds ratio, p-value, p-value category (as above), and trait characteristic (binary vs. quantitative) on replication. They initially split data into test and train sets (split, randomly, by half). Using the train set, they constructed logistic regression model using the following predictors: odds ratio (numeric, not category), p-value category, and trait characteristic (binary vs. quantitative). They then tested the constructed model on the test set.They report the model’s predictive accuracy via the following metrics: sensitivity, specificity, and area under the curve (AUC) all with 95% confidence intervals. They further assessed model fit via McFadden’s R2.

The overall replication rate was 85.0% (95% Confidence Interval (CI): 84.1% to 85.8%) and it was lower for binary than for quantitative phenotypes (58.1% (95%CI: 55.7% to 60.4%) versus 94.8% (95%CI: 94.2% to 95.4%) respectively). The replication rate varied across the included phenotypes from 52.7% to 99.6%. There was a 18.0% decrease in SNP effect size for binary phenotypes, but a 12.0% increase for quantitative phenotypes. Using the discovery SNP effect size, phenotype trait (binary or quantitative), and discovery p-value, they created a model that predicted SNP replication with area under the Receiver Operator Curve = 0.90 (95% CI: 0.89 to 0.91) corresponding to a sensitivity and specificity of 70.9% (95% CI: 69.2% to 84.5%) and 93.6% (95% CI: 80.0% to 95.6%) respectively. They found a McFadden’s R2 of 0.33. 

While non-replication may often reflect lack of power rather than genuine false-positive findings, these results provide insights about which discovered associations are likely to be seen again across subsequent GWAS. First, the SNP replication rate for quantitative phenotypes is very high; implying that quantitative GWAS in the UKBB had likely reached sufficient power to accurately detect all SNPs that were truly associated with a phenotype and that had been discovered by earlier GWAS efforts. The high replication rate observed for quantitative traits may also reflect the precision and relative ease in which quantitative traits can be measured. The converse of this, the likely measurement error and ultimate definition heterogeneity of binary phenotypes, may be one explanation for the relatively low rate of replication in binary phenotypes. For instance, binary phenotypes often represent complex clinical diseases that can have a) broad diagnostic criteria (e.g. angina, and myocardial infarction are often captured under “Coronary Artery Disease”) and b) are defined via an array of data sources, of varying quality. The UKBB, for instance, defines their phenotypes with ICD codes based on linked electronic health records (EHR). While this probably represents the best current method to define phenotypes in large cohorts, EHR data is “messy” and likely to include some “administrative and clinical error”.














<!--chapter:end:03-Genome-wide-association-analyses.Rmd-->

# Heritability

If you have an outcome that is the sum of effects from independent sources (like, say, genes and environment) the variance of the effects from each source add up to the variance of the outcome.

Talking about variance implicitly means we’re talking about a group or population of individuals.

When we say "the proportion of variation in a trait explained by", by explained we refer to the variance that could be predicted based on genetic data if we had perfect information about the effects of all genetic variants.

## Realized heritability

## Liability vs observed scale

### Evolvability

### Reliability

## Twin studies

## Haseman-Elston regression

## GREML

## GREML in family data

## GREML in WGS or imputed data

### GREMLd

### Bivariate GREML

## LD-score regression (LDSC)

Attenuation bias in LDSC analyses is essentially a measure of model misspecification: the basic assumption of LD score regression is that association chisquare statistics should (on average) increase linearly with the extent to which a SNP tags other potentially causal SNPs—i.e., the LD score of the SNP. If this model holds perfectly, then the regression has an intercept of 1 and an attenuation ratio of 0; on the other extreme, if LD scores are completely non-informative, the regression becomes flat with an intercept equal to the mean chi-square statistic and an attenuation ratio of 1. In general, most LDSC analyses exhibit modest nonzero attenuation ratios.

Attenuation ratios vary among traits (as expected, given that different traits have genetic architectures with different levels of agreement to the LDSC model); however, for a given trait, attenuation ratios are largely consistent between analyses of N=337K unrelated individuals vs. N=459K related individuals (also as expected, given that increasing sample size or relatedness does not affect the underlying genetic architecture of a trait). We also observe that attenuation ratios under the original LDSC model are generally larger than attenuation ratios under the baselineLD model, consistent with improved model fit upon augmenting the LDSC model with information about genomic annotations.

## LDAK

### SumHer

## Heritability by chromosome, MAF bin, or functional category

## Correction of heritability in presence of environmental effects




<!--chapter:end:04-Heritability.Rmd-->

# Genomic prediction

## Unweighted sum of risk alleles

## Polygenic risk scores

Among a range of applications, PRS are exploited to assess shared aetiology between phenotypes, to evaluate the predictive power of genetic data for use in clinical settings, and as part of experimental studies in which, for example, experiments are performed on individuals, or their biological samples (eg. tissues, cells), at the tails of the PRS distribution and contrasted.
As well as identifying shared aetiology among traits, PRS have been used to test for genome-wide GxE and GxG interactions, to perform Mendelian Randomization studies to infer causal relationships, and for patient stratification and sub-phenotyping.

If the population-level effects of the SNPs were estimated from the GWAS without error, then the PRS could predict the phenotype of individuals in the target data with variance explained equal to the “chip-heritability” (h2) of the trait (i.e. PRS provides an upper bound estimate of heritability). However, due to error in the effect size estimates and inevitable differences in the base and target samples, the predictive power of PRS are typically substantially lower than h2 but tend towards h2 as GWAS sample sizes increase.

Important challenges in the construction of PRS are the selection of SNPs for inclusion in the score and what, if any, shrinkage to apply to the GWAS effect size estimates.


## Gene-based polygenic score (POLARIS)

## Pathway-based polygenic risk score

## LD adjusted PRS

### LDpred with functional annotation

## Annopred

## Pleiopred

## Prediction including GxE

## BLUP

### GBLUP

### sBLUP

## Bayesian Zoo

### B

### C

### S

### N

### NS

### R

## Reproducing kernel Hilbert space

## Machine learning methods


<!--chapter:end:05-Genomic-prediction.Rmd-->

# Pleiotropy

## Fisher's geometric model

## Direct

## Indirect


<!--chapter:end:06-Pleiotropy.Rmd-->

# Gene-set analysis

## Gene-set conditional analysis

## Gene-set interaction analysis

<!--chapter:end:07-Pathway-analysis.Rmd-->

# Functional annotation

<!--chapter:end:08-Functional-annotation.Rmd-->

# Causal inference

## Gene-knockout

## Conditioning

Adding additional SNPs as covariates

## COJO

## mtCOJO

## Finemapping

## Mendelian Randomization

Mendelian Randomization relies on three assumptions about the instruments:

1) They must be sufficiently strongly associated with the exposure.
2) They should not be associated with any confounder of the exposure-outcome relationship.
3) They should be associated with the outcome only through the exposure.

Violation of any ofthese assumptions often through pleiotropy  would lead to biased estimates of the causal effect and potential false positives. The second assumption is the most difficult to verify because confounding factors are often unknown, whereas the last assumption can only be partially verified at best if SNPs, exposures and outcome data are available from the same (large) sample, a condition not fulfilled for two-sample MR.

These assumptions imply that the genetic variants (IVs) have a causal effect on the outcome only via the risk factor. While the first assumption can be easily verified, the second one is impossible to confirm as not all confounders are known, and the third requires instrument-exposure-outcome data measured in the same sample and is often violated by pleiotropy. One (ideal) way to guard against the violation of the third assumption is to use as many IVs possible, as the pleiotropic effect of each marker will cancel each other out under the INSIDE assumption (instrument strength independent of the strength of the pleiotropy).

Cochran’s Q test can be used in order to test for the presence of pleiotropy.

### Two-sample MR

### MR-Egger

### Bidirectional MR

### Weighted median

### Mode based estimator

### Summary data-based MR (SMR)

SMR can be used to test whether the effects of genetic variants on a phenotype are mediated by gene expression. The method is based on a single genetic variant.

### Generalised summary-data-based MR (GSMR) and HEIDI

GSMR was developed to overcome the issue of unaccounted pleiotropy in SMR method. This method performs a MR analysis with multiple independent SNPs as instrumental variables. 

### Joint analysis of GWAS and eQTL data

It is common in GWAS studies to identify variants that are located in genomic regions thought to be involved in regulation of gene expression. In order to improve power from GWAS studies, a method involving joint analysis of GWAS and eQTL data was proposed. This approach superceeds previous TWAS methods by allowing to account for pleiotropy in a Mendelian Randomization framework.

Two things are needed in order to carry out this test: summary-level data and pair-wise SNP LD information.

The method was developed after realisation that many pleiotropic effects are mediated through the expression levels of neighbouring genes; hence including other genes as exposures should reduce MR assumption violation, while improving power. Furthermore, such approach might be able to better distinguish the causal effects of genes with correlated expression levels.

As any other method, this approach suffers from several limitations:

1) The putative causal associations reported in this study are not definitive. They provide a prioritized list of candidate genes for future follow-up studies and also shed light on possible biological mechanisms of complex traits.

2) Currently, eQTL data is available for 15K egenes, which substantially decreases power to detect enrichment of prioritized gene-set in relevant pathways and regulatory networks.

3) Violations of MR assumptions. In particular, horizontal pleiotropy and indirect effects of the instruments on the exposures can substantially bias causal effect estimates.

Because the study requires summary statistics and LD pattern, it is possible to extend the method to include other omics information.

### Tissue-specific MR

Since many traits manifest themselves only in certain tissues, it is important to integrate data from the tissue of interest for the studied phenotype when trying to interpret GWAS results using gene expression as an intermediate phenotype. 

### Latent causal variable model

Inference of causality across traits.


<!--chapter:end:09-Inferring-causality.Rmd-->

# Combining multiple datasets

## Colocalization



## Meta-analysis

### Meta-analysis of gene-level associations (common)?

### Meta-analysis of rare variants

RAREMETAL and RAREMETALWORKER

## Reverse meta-analysis

## Mega-analysis


## Convert Z-statistic to estimated SNP effect

After sample-size-weighted meta-analysis, Z-statistics can be transformed into unstandardized regression coefficients using the following equation:

$$\LARGE \hat{\beta_j} = Z_j\frac{\hat{\sigma_Y}}{\sqrt{2N_j MAF_j(1-MAF_j)}}$$
for SNP *j* with minor allele frequency MAF*j*, sample size N*j*, Z-statistic Z*j*𝑍𝑗, and standard deviation of the phenotype $\hat{\sigma_Y}$𝑌. 

<!--chapter:end:10-Analysis-of-multiple-datasets.Rmd-->

# Gene-environment interaction

It is generally accepted that complex diseases are caused by an interplay of genetic and environmental factors, creating a challenge for understanding the disease mechanisms. Understanding the interplay between genes and environmental factors is important, as genes do not operate in isolation but rather in complex networks and pathways influenced by environmental factors.
Identification of gene-environment interactions has important implications for understanding underlying disease etiology and developing disease prevention and intervention strategies.
In addition to providing insights into disease etiology, exploiting gene-environment (G-E) interaction can help discover novel susceptibility loci for complex diseases, where genetic effects are modified and masked by the effects of environmental factors.
From a public health perspective, G-E interaction is useful because findings based on interactions can help develop strategies for targeted intervention; conducting an intervention focusing on a subset of the population identified by G-E interactions can provide efficiency in disease prevention.
There are several challenges of G-E interaction analysis that include replication issues. While more powerful statistical methods for detecting interactions are helpful, ultimately studies with larger sample sizes are needed to identify interactions through consortium-based studies to achieve adequate power for G-E analysis.

This chapter aims to describe methods that extend single marker regression by taking into consideration the effect of gene-environment interaction.

## Single step methods

There are several disease risk models for the joint effects of G and E, and interpretations of G-E interactions depend on the underlying disease risk models.

### Case-control

### Prospective likelihood-based approach

### Retrospective likelihood approach

To address the limitations of case-only approaches that can only test for multiplicative interactions (not for the main effects of G and E), Umbach and Weinberg (1997) generalized the case-only design idea to use a log-linear model based on case-control data. They showed the maximum-likelihood estimates for all parameters of a logistic regression model can be obtained using a log-linear model. Along the same line, Chatterjee and Carroll developed a general method using a retrospective likelihood that exploits the G-E independence assumption to test for multiplicative interaction, but can use both cases and controls to estimate all of the parameters in a general logistic regression model. Basically, this method employs a retrospective likelihood that explicitly models the conditional probability of G given E mediated by an association parameter θ that can be constrained to be zero when the G-E independence assumption holds. This likelihood can be used for testing both multiplicative and additive interactions; recently, Han et al. developed a likelihood ratio test that exploits the G-E independence assumption using a retrospective likelihood. Their numerical investigation of power suggests that the incorporation of the independence assumption can enhance the efficiency of the test for additive interaction by 2- to 2.5-fold.

#### Multiplicative scale

A multiplicative model incorporating GxE effects is one of the most commonly used models via logistic regression: logit(Pr(*D* = 1|*G*,*E*)) = *β<sub>o</sub>* + *β<sub>g</sub>G* + *β<sub>e</sub>E* + *β<sub>ge</sub>GE*, where *G* is a genotype of a SNP, *E* is an environmental risk factor, and *D* is the disease status. 

Assuming binary factors for both G and E, a 2 × 2 table for a disease risk for each combination of G and E values can be constructed based on this model (Table X).

#### Additive scale

An additive model is shown as logit(Pr(*D* = 1|*G*,*E*)) = *b<sub>o</sub>* + *b<sub>g</sub>G* + *b<sub>e</sub>E* + *b<sub>ge</sub>GE*, where the effects of G and E are additive on the disease risk scale, but not on the logit scale. 

### Case-only

This design depends on an assumption of G–E independence in the underlying population. Under the assumption of G-E independence in the underlying population (i.e., controls), a multiplicative interaction test statistic becomes equivalent to testing the association between G and E among cases. One major limitation of the case-only design is that while the case-only method has improved power over the traditional methods when G and E are independent in the underlying population, this method has an increased type I error if the independence assumption is violated. In addition, the regression parameters for the main effects of G and E cannot be estimated using this method because the case-only test is only for evaluating a multiplicative interaction.

### Empirical Bayes and Bayesian Model Averaging

### Other Bayesian approaches

#### Interactions using haplotypes

First, haplotypes are biologically relevant. There is strong evidence that several mutations within a gene may interact ( cis-interaction) to cause a disease. Second, the power of single-marker-based methods in association studies depends on LD between the tested marker and the disease susceptibility locus. LD information contained in flanking markers is generally not incorporated, which can result in a reduction in power. In addition, haplotype-based methods can be more powerful when multiple disease susceptibility alleles occur within a single gene.

Lake et al.  [21]  proposed a likelihood-based method in the generalized linear model framework, which has been commonly employed in haplotype-based association studies, because it is publicly available and easy to implement with its R package. This approach, however, is limited by ignoring the interacting effects between haplotype blocks. Subsequently, several methods have been developed to study haplotype-related interactions, but these methods do not consider all potential haplotypes and interactions simultaneously  [22–27] . Recently, Guo and Lin  [15]  proposed a generalized linear model with regularization to detect interacting haplotype effects. However, their method applies an omnibus test and consequently does not provide inference on the effects of specific haplotypes and their interactions. Another concern for haplotype-based methods is that large numbers of haplotypes inferred from genotype data  [28–30]  often lead to high degrees of freedom for corresponding statistical tests and thus reduce power  [14, 15, 31–34].  If interacting effects are considered, such problems become severer. Unfortunately, few statistical methods have been developed to tackle these farreaching problems in the study of haplotype interactions.  One challenge in haplotype-based association studies is that for haplotypes comprising multiple markers, there might be many rare haplotypes. Because of their low frequencies, the parameter estimates related to rare haplotypes will have large variances, leading to unstable models. Schaid  [6]  described several approaches to handle rare haplotypes. One approach is to combine all rare haplotypes into one group or rare haplotypes with common ancestral haplotypes, and another is to exclude them from the model, which is equivalent to including them in the baseline group. However, both approaches yield results that may be difficult to interpret. In addition, it has been argued that rare haplotypes may account for a substantial fraction of the multifactorial inheritance of common diseases  [35–39],  thus the aforementioned approaches may miss the rare haplotypes having true effects. Another approach is to include the effects of each rare haplotype in the model but shrink their effects towards the common mean or towards the effect of a similar haplotype  [6, 15, 40].  Recently, Guo and Lin  [15]  adopted a lasso penalty in their generalized linear model to allow assessment of the effects of rare haplotypes by decreasing the coefficients of unassociated haplotypes to zero so that the associated ones, especially those that are rare, can be estimated. 

The estimate of haplotype dosage is the estimate of the number of copies of a specific haplotype for a subject. For the haplotypes that can be unambiguously resolved based on the observed genotype data, the values of haplotype dosage of a haplotype for a subject can be 0, 1, or 2. But for the haplotypes that cannot be unambiguously resolved, the values of haplotype dosage of a haplotype for a subject would be non-integer, ranging from zero to two, reflecting the possibility that haplotypes are based on the subject’s genotypes. For each subject, the sum of haplotype dosage across all haplotypes is equal to two.
 
## GxE in the context of family studies

## Multi stage methods

Several approaches exist. In general, these methods suggest selecting a subset of SNPs based on the marginal effects of SNPs or G-E correlation tests in the first stage and conducting standard G-E interaction tests in the second stage, where the independence between the test statistics used in the two stages is required to provide a valid screening procedure.

For calculating power for G-E interactions, the powerGWASinteraction R package is available (https://cran.r-project.org/web/packages/powerGWASinteraction/index.html), which includes a power calculation tool for four two-stage screening and testing procedures.

## Joint tests

### Gene-based

## Set-based interaction tests

There are multiple reasons for using set-based gene-environment interaction tests. 

1. Multiple comparison adjustments for a large number of markers across the genome could result in power loss.
2. Closely located SNPs are correlated because of linkage disequilibrium. Multiple tests for GxE in these single-marker-based GxE models are even more dependent, as interaction terms in these models share the same environmental variable. Dependence among multiple tests can result in incorrect Type 1 error rates and causes bias in standard multiple comparison adjustments and this bias is often difficult to correct.
3. The single-marker GxE test does not interrogate the joint effects of multiple SNPs that have similar biological functions. When the main effects of multiple SNPs in a set are  associated with a disease/trait, the classical single marker regression interaction test can be biased.

Lin et al. (2013) developed a method to analyse GxE for a set of markers using generalized linear mixed models. The method tests for SNP-set by environment interactions using a variance component test, and because a set of variants will likely be correlated, the main SNP effect estimates under the null hypothesis are obtained using ridge regression. 
Their software is called GESAT. Here, they model GxE effects as random, as opposed to the classical approach of treating *β<sub>j</sub>*’s as fixed effects followed by a test with *p* degrees of freedom. The latter approach can suffer from power loss when *p* is moderate/large, and numerical difficulties when some genetic markers in the set are in high LD. 
The model allows to adjust for the main effects of all SNPs while simultaneously testing for the interactions between the SNPs in the region and environmental variable. For unbalanced designs when a binary environmental exposure has a low frequency in one category, GESAT is most advantageous over single marker regression GxE test. Such unbalanced designs can occur due to case–control sampling and the strong association of an environmental factor with disease. When the effect size is modest, GESAT performs better that single marker regression GxE test, but when the effect size is strong, the opposite is true. 
Their simulations suggest that the power of GESAT seems fairly robust to the dependence between G and E.

The same approach can be applied to investigating various other biological problems. For example, we can test for the interactions between gene expressions in a pathway or network and an environmental variable by simply replacing G by gene expressions in a gene-set.

Existing methods for assessing common variants by environment interactions such as Gene-Environment Set Association Test (GESAT) (Lin et al., 2013) have several limitations when applied for rare variants. GESAT estimates the main effects of the common variants by applying a L<sub>2</sub> penalty on the genotypes scaled to unit variance; this assumes that the main effects of the scaled genotypes are comparable in magnitudes, which may not hold in the case of rare variants. GESAT also assumes that the regression coefficients of the rare variants by environment interactions are independent of each other, and suffers from power loss when most rare variants in a gene interact with the environmental factor and the interaction effects have the same direction.

Similar idea from GESAT was later extended and applied to analyse rare variants. GESAT have several limitations when applied for rare variants. GESAT estimates the main effects of the common variants by applying a L<sub>2</sub> penalty on the genotypes scaled to unit variance; this assumes that the main effects of the scaled genotypes are comparable in magnitudes, which may not hold in the case of rare variants. GESAT also assumes that the regression coefficients of the rare variants by environment interactions are independent of each other, and suffers from power loss when most rare variants in a gene interact with the environmental factor and the interaction effects have the same direction. The new proposed test iSKAT is optimal in a class of variance component tests and is powerful and robust to the proportion of variants in a gene that interact with environment and the signs of the effects. This test properly controls for the main effects of the rare variants using weighted ridge regression while adjusting for covariates.

A naive approach to assess rare variants by environment interactions is to extend the burden test by fitting a model with both the summary genetic burden variable, environment, and their interaction, and performing a one degree of freedom test for the interaction. However, when there are multiple causal variants with their main effects having different magnitudes and/or signs, such a burden rare variant by environment test fails, and may lead to inflated Type 1 error rates. This is because adjusting for the main effects of the multiple causal variants using a single summary genetic burden variable is inappropriate. Likewise, a naive approach to assess rare variants by environment interactions using SKAT by including the main effects of rare variants as part of covariates and applying SKAT to the interaction terms is problematic. This is because SKAT only allows adjustment of a small number of covariates and cannot handle the presence of a large number of rare variants in a region. Furthermore since the rare variants are observed in low frequency, a model with all the rare variants as main effects will be highly unstable and may not even converge.

Both GESAT and iSKAT are able to incorporate multiple environmental variables.

No gene-based GxE test exists for the analysis of the sex chromosomes as of writing this text.

The rareGE R package (https://www.hsph.harvard.edu/han-chen/software/) provides various functions for detecting G-E interaction as well as for testing the joint effect of a gene and G-E interaction under a set-based framework. 
The SIMreg R package (http://www4.stat.ncsu.edu/~jytzeng/software_simreg.php) offers functions for testing a set-based G-E interaction by using genetic similarity to aggregate information across SNPs, and incorporating adaptive weights depending on allele frequencies to accommodate rare and common variants.

https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12428

https://academic.oup.com/biostatistics/article-abstract/18/1/119/2555344?redirectedFrom=fulltext

### Combining multiple environmental factors

### Multi-trait multi-GxE tests

### Variance heterogeneity

All methods described in this chapter so far conduct a location (mean) based test. In other words, the main concern is whether there is a difference in the mean effect between genotypes. Testing for scale (variance) heterogeneity could be used to identify novel variants involved in the control of the phenotype possibly through interactions with other genetic variants or environmental factors. This is advantageous when the interacting variable was not collected and the interaction term may not be directly modeled or when a phenotype was transformed. Therefore, the analysis of variances of the trait is based on SNP information only. The presence of interaction between a genotype and certain factor is expected to alter the trait's variance in the group of subjects carrying such genotype.

Yang et al. (2012) noticed that the mapping of variance-controlling loci is prone to inflated test statistics when the minor allele frequency (MAF) is small.

One way of interpreting an increase in variability is as a decrease in stability. Waddington (1942) described the concept of canalization, whereby natural selection favors the relative constancy of some attributes, for example, well-formed organs and limbs, and thereby leads to the evolution of heritable architectures that buffer the impact of environmental or background genetic variation that would otherwise cause development to go astray. These architectures create virtual “canals” down which developmental programs flow. For a canalized phenotype, which modern usage expands to include nondevelopmental traits, the “zone of canalization” is the range of underlying liability over which potentially disruptive variation may be absorbed without serious consequence to the expressed trait value (Lynch and Walsh 1998). A well-studied example of a stabilizing architecture is that provided by heat-shock protein 90 (Hsp90), which buffers genetic and stochastic variation in the development of plants and flies (Rutherford and Lindquist 1998; Queitsch et al. 2002; Sangster et al. 2008).

But in absorbing variation, such stabilizing architectures also hide it from view, and a sensitizing change in the stabilizer that shifts liability outside the zone of canalization can have a dramatic effect on the phenotype. Such shifts release the combined effects of previously “cryptic” genetic variation: now decanalized, the phenotype is more sensitive to internal (including genetic) and external environment, and as a result varies more greatly between individuals (Dworkin 2005; Hornstein and Shomron 2006). In this vein, decanalization has been proposed to explain why the genetic architectures of some diseases in human populations seem more amenable than others to genetic dissection through genome-wide association (Gibson and Goldstein 2007). Specifically, whereas some disease phenotypes in homogeneous populations may be heavily canalized and thereby harder to dissect, others may have been decanalized by modern living conditions (e.g., inflammatory diseases) or modern admixture, while yet others are simply too recent in evolutionary history for buffering networks to have evolved (e.g., response to HIV). Increased variability can also be adaptive. In natural populations disruptive selection favors diversity, with increased “capacitance” (Rice 2008) or “bet-hedging” (Beaumont et al. 2009) spreading risk over a variable fitness landscape. Feinberg and Irizarry (2010) recently proposed a heritable and selectable mechanism for this based on stochastic epigenetic variation. In controlled populations, variability can be increased through directional selection. For example, in a Drosophila selection experiment Clayton and Robertson (1957) reported increased bristle number variance, which is consistent with the idea that genotypes associated with higher environmental variance have a greater chance of being selected under directional selection (Hill and Zhang 2004). Moreover, genetic differences have been observed for phenotypic variability in body weight for chickens (Rowe et al. 2006) and snails (Ros et al. 2004) and litter size in rabbits (Ibanez-Escriche et al. 2008), sheep (Sancristobal-Gaudy et al. 1998), and pigs (Sorensen and Waagepetersen 2003). In natural populations with stabilizing selection we should expect to find alleles minimizing variance for fitness traits (Lande 1980; Houle 1992), whereas directional selection during domestication will favor alleles that increase variance. One may therefore expect to find vQTL in experimental crosses between wild and domestic animals (see Andersson 2001). Nonetheless, genetic buffering that leads to phenotypic robustness need not require an evolutionary explanation to be observed, nor to be useful in medicine and agriculture. Plainly, detecting vQTL and inferring how they arose are separate questions; here we concentrate on the first.

Given that a genotypic effect at one locus may be affected by the genotype at an adjunct locus or some environmental factor, the genotypic effect at that locus is a composite of multiple distributions with different means, resulting in an inflated variance for that genotype (Struchalin et al., 2010; Deng and Par´e, 2011). In addition, it has been shown that vQTL can be induced by linkage disequilibrium with nearby functional locus with mean effects only (Cao et al., 2014).

### Levene's test

Levene’s test (Levene, 1960) is known for its simplicity and robustness to modeling assumptions, and it is perhaps the most popular method for evaluating variance heterogeneity between *k* groups. It's performance is best when the genetic loci have only variance effects on quantitative traits (Hong et al., 2016). More recently it has been employed as an indirect test for interaction effects.

### Brown-Forsythe test (also known as Levene's median test)

$$\LARGE T^2 = \frac{(N-k) \sum_{j=0}^{k-1} n_j(Z_j.-Z_..)^2}{(k-1)\sum_{i=1}^{N}(Z_i-Z_gi.)}$$

The Brown-Forsythe is a statistical test for the equality of group variances and is based on
an ANOVA of the absolute deviation from the median. It has earlier been shown to be robust to deviations from normality of the phenotypic distribution in GWAS applications.

### Bartlett's test

$$\LARGE T^2 = \frac{(N-k)ln(\sigma_P^2)- \sum_{j=0}^{k-1} (n_j-1)ln(\sigma_j^2)}{1+\frac{1}{3(k-1)}(\sum_{j=0}^{k-1}(\frac{1}{n_j-1}-\frac{1}{N-k}))}$$
where *k* is the number of genotypes tested,

### Bartlett's test with prior rank transformation to normality

### Generalized Levene's scale tests

The Levene's test for variance heterogeneity was expanded to include sample correlation and group membership uncertainty (genotype imputation probabilities). Following a two-stage regression framework, it was shown that the least absolute deviation regression must be used in the stage 1 analysis to ensure a correct asymptotic χ<sup>2</sup><sub>k−1</sub>/(k−1) distribution of the generalized scale (gS) test statistic. 
gS has good type 1 error control in the presence of sample correlation, small samples, unbalanced group sizes, and non-symmetric outcome data.

Same study showed that the least absolute deviation (LAD) regression approach to obtain group-median-adjusted residuals is needed to ensure robust performance of gS (and possibly LAD should be used in all scale dependent tests).

Several improvements including adjustment for covariates could be further explored (check original paper)

### The Lepage test

The Lepage test is a rank-based non-parametric test for either mean or variance difference, which combines the Kruskal–Wallis test statistic for mean difference and the Fligner–Killeen test statistics for variance difference, and has been shown to be powerful and robust when the locus has both mean and variance effects (Lepage, 1971; Hollander and Wolfe, 1999).

### The *D*-test

Aschard et al. (2013) introduced a general distribution-based test or *D*-test, which can be thought as an extension of the classical Kolmogorov–Smirnov test (Brittain, 1987). There are two versions of *D*-test, namely, the constrained *D*-test (*D*<sub>c</sub>) and the unconstrained *D*-test (*D*<sub>u</sub>). The *D*<sub>c</sub> test is designed for the situation when the effect of the variant is monotonic, while the *D*<sub>u</sub> test has the ability to detect genetic effects in a broader range of situations (Aschard et al., 2013).

### Regression using the squared Z-score

### Gamma regression models

### Two-step screening on residual variance heterogeneity

#### YGVH

One caviet involving VH tests is that the absence of VH for a SNP can not be interpreted as the absence of the SNP involvment of the SNP in the interaction network.

When there are only two genotype classes, the type I error rate can be very large if the number of minor genotypes contains fewer than 100 observations when using regression on the squared Z-score, and this cannot be overcome by increasing the total sample-size. The Levene and Brown–Forsythe tests also show such an inflation of false positives, but use of a Gamma regression model, which accounts for the fact that the squared Z-score follows a chi-square distribution, overcomes this problem. Populations with three genotypes will, in practice, be more robust when the allele substitution model implemented in most GWAS-software is used (i.e., when regression on all three genotypes is used to estimate the additive effect). Inflated type I error rates are then observed only when the intermediate-size genotype class (i.e., in practice most often the heterozygotes) contains fewer than 100 individuals.

## Mean-variance QTL (joint test)

It has been suggested that many loci have both mean and variance effects, while some of the mean or variance effects alone would not be strong enough to be detected (Shen et al., 2012; Cao et al., 2014).

One approach to detect both mean and variance effects includes double generalized linear model (DGLM). DGLM can be used to model effects of the mean and variance simultaneously, thus enhacing the power to detect variants associated with the trait. This approach is able to not only incorporate genetic and covariate effects for the mean but also set of such effects for the residual variance.

Extensions to family-based data have been developed (Cao et al., 2014).

### Conditional quantile regression 

In contrast to OLS regression, quantile regression tests for the effect differences across the range of quantiles. More specifically, conditional quantile regression (CQR), can be used to model effect size changes across the sample distribution. This method is similar to other varaince heterogeneity methods in a way that does not require interacting effects to be known and hence included in the model. The downside of this is that further investigation is required to identify environmental factors causing statistical interactions.

If the marginal and interaction effects are in opposite directions, then the power to detect potential interactions is reduced. Hence, failure to identify differences in variance by genotype effects does not rule out interactions effects.

Tests for variance heterogeneity detect variance inconsistency rather than variance structure (i.e. direction of change). Therefore, variance heterogeneity is not specific to interaction signals, but includes conditions where no consistent direction for increasing or decreasing variance per genotype is observed. Modeling relationship between variance and genotypes assuming a structure (i.e. linear trend) could help improve power to detect variants with potential interactions if the assumtions are met.

### Sliced inverse regression

Jiang and Liu, 2014

### Semiparametric model for vQTL mapping

The semiparametric model is designed to identify the combined differences in higher moments among genotypic groups. The pairwise likelihood is constructed based on conditioning procedure, where the unknown reference distribution is eliminated.

### Squared residual value linear modeling (SVLM)

SVLM consists of two steps. First, a regression analysis is applied where the trait is adjusted for a possible SNP effect and other covariates. Second, a regression analysis is applied to the squared values of residuals obtained from the first stage, using the SNP as the predictor.

## RELIEF and other machine learning tools

### Multidimensionality reduction

## Meta-analytic GxE approaches

## Gene-environment correlation

## Other challenges

There are several challenges of G-E interaction analysis. One main challenge is replication issues. While various GWAS findings of the main effects of SNPs have been replicated by independent studies for many complex diseases (http://www.ebi.ac.uk/gwas/), relatively few interactions have been reproduced. It is likely that the sample sizes of GWAS that have required measurements on environmental exposures are not yet adequate to reliably identify G-E interactions of modest magnitude. In addition, differences in the underlying distribution of environmental exposures across various studies as well as difficulties in accurately measuring environmental exposures can also lead to reduced power of detecting G-E interactions. While more powerful statistical methods for detecting interactions are helpful, ultimately studies with larger sample sizes are needed to identify interactions (e.g., through consortium-based studies) to achieve adequate power for G-E analysis. A reasonable goal for the future will be to at least identify parsimonious models that adequately describe the risks of diseases associated with a combination of genetic and environmental risk factors. The lack of reporting of interaction in current studies so far indicates that linear logistic models, i.e., multiplicative models, in general may be a good starting point for building models for evaluating the joint effects of genetic and environmental factors.

When using a test designed to detect variance heterogeneity, it is of vital importance to ensure that phenotype transformations do not lead to false positive associations. For an autosomal diallelic SNP in particular, it is difficult to make valid biological inferences about a quantitative trait on the basis of variance heterogeneity among the genotype-specific distributions unless the locations (in particular, the means) of the three distributions are equal. When the means are not all equal, the variance heterogeneity might in fact be explained by a distribution in which the variance is a function of the mean. There exists a transformation of the data that will tend to make the variances equal and there is a method that will at a minimum decrease the variance heterogeneity.

GWAS analyses to detect variance heterogeneity is inherently sensitive to unbalanced data. The major reason for this is that the distribution of the variance often deviates from normality as it: 
1) is bounded at zero 
2) has a distribution skewed to the right 
3) has a variance depending on its mean 
Such deviations lead to violations of, e.g., the Gauss–Markov assumptions in a regression model.

One disadvantage of using variance heterogeneity methods is that the underlying interacting factors are unknown. Given that shifts in variance could be caused by the incomplete LD between the causal polymorphism and the tested marker, multiple functional alleles, gene-gene or gene-by-environment interactions, it could be difficult to identify the root cause of phenotypic variability. Transformations on a phenotype can also result in variance heterogeneity (Sun et al., 2013). This transformation can occur knowingly for statistical purposes, for example, log(Y), or unknowingly, for example, choosing a phenotype measurement that does not directly represent the true underlying biological outcome of a gene.

### Controlling for covariate interactions

In order to eliminate potential confounders, it is common to add covariates such as sex and ethnicity in the model. While this provides control for their potentially confounding influences on the main effects of the genotype and the environment, in the case of GxE studies, this practice does not control for the effects these variables might have on the G×E interaction. Rather, to properly control for confounders, researchers need to enter the covariate-by-environment and the covariate-by-gene interaction terms in the same model that tests the G×E term.

The GxE interaction will be biased in the model without the covariate-by-environment and the covariate-by-gene interaction terms if:

1) The covariate is related to the genetic variable and the covariate-by-environment interaction coefficient is nonzero.
2) The covariate is related to emvironmental variable and the covariate-by-gene interaction coefficient is nonzero.

Finally, it should be noted that even if a G×E result ‘disappears’ after properly controlling for covariates, this does not necessarily mean that the original G×E hypothesis was wrong. For example, the genetic polymorphism might cause changes in the covariate which in turn moderates the environmental variable, in which case the covariate is a mediating mechanism by which the gene moderates the environmental variable. That said, this possibility applies to all models that statistically control for covariates in regression, and the traditional interpretation of ‘disappearing’ effects after controlling for a covariate is that the true causal pathway is ambiguous and alternative (confounding) explanations cannot be ruled out. That said, in some cases, a particular causal pathway can be discarded as impossible or unlikely. In such cases, investigators can be more definitive about ruling out certain hypotheses. For example, changes at a genetic polymorphism will not lead to changes in ethnicity, and so a G×E hypothesis can be safely discarded if it is mediated by an ethnicity-by-environment interaction.

## Candidate gene-by-environment interaction studies

This research area has been a hot topic in genetics, with hundreds of publications reporting positive G×E discoveries over the last 15 years, but there has been increasing skepticism about the validity of many of these findings. This skepticism is based on a number of substantive and statistical concerns:
t
1) A low replication rate among attempted direct replications of GxE findings.
2) The possibility that GxE findings capitalized on chance from among many unreported analyses.
3) A publication bias toward positive findings.
4) Small sample sizes that exacerbate the already low statistical power for detecting interactions.
5) The low prior probability that a specified environmental variable interacts with a specified candidate gene polymorphism.

## Higher order interactions

It is difficult and potentially misleading to interpret two-way interactions in the presence of three-way interactions. In such a model, the lower-order two-way interactions become conditional interactions, and the regression betas and p-values are interpreted as the predicted two-way interactions when the other (omitted) variable is coded as 0.

Add more

<!--chapter:end:11-GxE.Rmd-->

# Gene-gene interaction

When the combined phenotypic effect of alleles at two or more loci deviates from the sum of their individual effects, this is referred to as a genetic interaction, or epistasis. 
There are some situations where data and theory have suggested that it might be particularly important to account for genetic interactions. One is when the aim is to predict the phenotypes of individuals on the basis of their genotype. If interactions lead to extreme phenotypes for some genotypes, these phenotypes are unlikely to be captured by additive models, particularly if they are rare. Another case is the prediction of long-term selection response. Under an additive model, both the additive variance and the response are expected to be nearly constant over the first few generations. As generations proceed, allele frequencies change to alter the additive variance and, consequently, the response to selection. This change is more rapid for traits regulated by fewer loci with larger effects than for traits regulated by many loci with smaller effects. It is known that genetic interactions can contribute to the additive genetic variance in a population. The contribution, however, varies depending on the joint allele frequencies across all the interacting loci as well as on the types and strengths of the genetic interactions. The changes in the additive variance, and hence the response, during ongoing selection are therefore more complex in the presence of genetic interactions.

Most genetic variance in a population is expected to be additive even in the presence of extensive epistasis. The lack of empirical knowledge about the pervasiveness and strength of epistasis in the genetic architectures of complex traits makes it largely unknown how much of the observed additive genetic variance in quantitative genetics studies is due to genetic interactions.

Epistatic gene action, namely when the effect of an allele at one locus varies depending on the genotype at another locus, is therefore not directly proportional to the level of epistatic variance in a population. This is because it will usually contribute to both the additive and epistatic genetic variances (Goodnight, 1988; Cheverud and Routman, 1995; Mackay, 2014). To what extent epistatic gene action will contribute additive genetic variance is determined by allele frequencies, the type of genetic interactions, and how the genetic models used are parameterized (Cheverud and Routman, 1995; Hill et al., 2008; Huang and Mackay, 2016).

## Single step methods

## Multi stage methods

## Machine learning methods

## Variance heterogeneity

The details for this method have been described in the previous chapter.

To identify an individual locus that makes direct contributions to the trait variance, a statistical test is used to identify significant differences in the phenotypic variance between the groups of individuals that carry alternative alleles at the locus. When such a variance difference exists between the genotypes at a locus, the locus displays a genetic variance-heterogeneity. 

The concept that the trait variability could also be under direct genetic control was introduced already in the 1940s when Waddington presented the idea of canalization, where he suggested that natural selection could act to produce traits that are robust to environmental and genetic perturbations (Waddington, 1942). He partly based his ideas on the observation that natural populations often are less variable than artificial populations of the same species. More recently, Hill and Mulder (2010) proposed that ‘the environmental variation’ can be regarded as a phenotype in its own right. One can then invoke much of the quantitative genetics methodology to search for genetic determinants of this phenotype. They consequently called this phenomenon ‘genetic control of the environmental variation’, a terminology which implies that it is the randomness, or instability, of the trait that is genetically controlled. Several studies have recently mapped individual loci where the different alleles affect not only the mean, but also the variance of traits (Hill and Mulder, 2010; Rönnegård and Valdar, 2012; Shen et al., 2012). These loci can be detected since the variability of the measured trait differs between groups of individuals that carry alternative alleles at the locus. A simple example would be two groups of humans, where the group of individuals homozygote for a certain allele include both very short and very tall individuals, while the second group that is homozygote for the alternative allele include individuals of similar height. This would lead to genetic variance heterogeneity between the two groups of individuals. Note that the mean height does not have to be different between the groups in order for this to occur.

<!--chapter:end:12-GxG.Rmd-->

# Other omics

## Transcriptome-wide association studies

### cis eQTLs

### trans eQTLs

### 3-D structure of the genome

## Phenome-wide association studies

## Metabolomics

## Epigenomics

<!--chapter:end:13-Multi-omics.Rmd-->

# Quantitative trait loci mapping



<!--chapter:end:14-QTL-mapping.Rmd-->

# Additional points of interest

## Defining novel locus

1. Summarise GWAS associations
(a) Clump associations to define bp-range for each locus (use LD from cohort used to run GWAS).
(b) Identify conditionally independent associations within each locus.

2. Check for locus novelty (“Has this locus previously been associated with this phenotype?”), i.e. does any variant within the locus bp-range have a genome-wide significant association in any previous relevant GWAS?
(a) No – the locus and signal are novel.
(b) Yes – the locus is not novel (but signal may be).
(c) Yes, but previously reported lead variant lies outside locus – probably novel but requires checking.

3. If the locus is not novel, check for novelty of signal (“Has this precise association signal been observed previously?”).
(a) Is there a direct look-up for the variant in previous sets of summary stats? If so, use that.
(b) If there is no direct look-up, select a proxy in the ancestry of the previous GWAS. We are seeking to get the best possible measure of the association in that ancestry.
Using that ancestry, look-up the LD between the lead variant(s) and all possible variants within the locus. Identify the variant with the highest LD r<sup>2</sup> that is present in the previously reported summary statistics, and use that as the proxy:
(i) If the variant is not present (or has very low MAF), so that no good proxy is possible, report this;
(ii) otherwise, report the summary statistics for the proxy;
(c) If the look-up is not genome-wide significant (i.e. possibly novel), check the LD of the signal with the lead SNP in the previous GWAS (using the LD for the ancestry of the previous GWAS).


## Phenotype transformation

### logs

### RINT

### Regression on residuals

Frisch–Waugh–Lovell theorem

set.seed(12345)
n <- 5000
e <- rnorm(n)
cov <- rnorm(n) + rbinom(n,1,0.3)
snp <- rbinom(n,2,0.1)

y <- cov + snp + e

Fit model with OLS:
summary(lm(y ~ cov + snp))
summary(lm(y ~ cov + snp))$coefficients[3,4]

Regression on residuals:
residuals = residuals(lm(y ~ cov))
summary(lm(residuals~snp))
summary(lm(residuals~snp))$coefficients[2,4]

## Kinship matrix

### Path coefficients

## Genetic relationship matrix

## Animal models

## Phasing

### Switch rate

## Haplotyping

## Statistical power

### Quanto

### GCTA-GREML

### Mendelian Randomisation

### Twin design

### When marker is a disease susceptibility locus

### When marker is not disease susceptability locus

## Multiple comparisons

### Effective number of independant variants

## Biases

### Ascertainment

### Attenuation

### Selection

## Correction for biases

Heckman correction

## Family studies

### Transmission disequilibrium tests

## Twin studies

## Adoption studies

## Equifinality (many genes give same trait)

## Gene dosage

### Allelic dosage

## Allelic heterogeneity

## Genetic heterogeneity

## Genomic imprinting

## Penetrance/phenocopy

## Endophenotypes

## Ploidy

## Extended phenotype

## Genome sizes

## cis-eQTL vs trans-eQTL

Compared to cis-eQTLs, trans-eQTLs typically have weaker effect size and less direct effect. As a result, they are more prone to violate assumptions in MR studies.






<!--chapter:end:15-Extra.Rmd-->

# (PART) Population Genetics {-}

# Genetic drift

<!--chapter:end:16-Genetic-drift.Rmd-->

# Mutation


## Mutation age

<!--chapter:end:17-Mutation.Rmd-->

# Selection

## Directional

## Balancing

### Frequency-dependent selection 1

### Frequency dependent selection 2


## Background selection

<!--chapter:end:18-Selection.Rmd-->

# Migration

<!--chapter:end:19-Migration.Rmd-->

# Diversity

<!--chapter:end:20-Diversity.Rmd-->

# Admixture

<!--chapter:end:21-Admixture.Rmd-->

# Linkage disequilibrium

<!--chapter:end:22-LD.Rmd-->

# In breeding and heterosis

<!--chapter:end:23-Inbreeding-and-heterosis.Rmd-->

# Assortative mating

<!--chapter:end:24-Assortative-mating.Rmd-->

# Identity

## IBS

### Long runs of IBT

## IBD

## IBT


<!--chapter:end:25-Identity.Rmd-->

# Neutral theory of molecular evolution

## Nearly neutral theory of molecular evolution

<!--chapter:end:26-Neutral-theory.Rmd-->

